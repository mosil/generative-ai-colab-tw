{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "oXnEutuDQa9c"
            },
            "outputs": [],
            "source": [
                "# Copyright 2024 Google LLC\n",
                "#\n",
                "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
                "# you may not use this file except in compliance with the License.\n",
                "# You may obtain a copy of the License at\n",
                "#\n",
                "#     https://www.apache.org/licenses/LICENSE-2.0\n",
                "#\n",
                "# Unless required by applicable law or agreed to in writing, software\n",
                "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
                "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
                "# See the License for the specific language governing permissions and\n",
                "# limitations under the License."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "JAPoU8Sm5E6e"
            },
            "source": [
                "# 使用 Gen AI SDK 開始使用 Multimodal Live API\n",
                "\n",
                "\n",
                "<table align=\"left\">\n",
                "  <td style=\"text-align: center\">\n",
                "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/multimodal-live-api/intro_multimodal_live_api_genai_sdk.ipynb\">\n",
                "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> 在 Colab 中開啟\n",
                "    </a>\n",
                "  </td>\n",
                "  <td style=\"text-align: center\">\n",
                "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fmultimodal-live-api%2Fintro_multimodal_live_api_genai_sdk.ipynb\">\n",
                "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> 在 Colab Enterprise 中開啟\n",
                "    </a>\n",
                "  </td>\n",
                "  <td style=\"text-align: center\">\n",
                "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/multimodal-live-api/intro_multimodal_live_api_genai_sdk.ipynb\">\n",
                "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> 在 Vertex AI Workbench 中開啟\n",
                "    </a>\n",
                "  </td>\n",
                "  <td style=\"text-align: center\">\n",
                "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/multimodal-live-api/intro_multimodal_live_api_genai_sdk.ipynb\">\n",
                "      <img width=\"32px\" src=\"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg\" alt=\"GitHub logo\"><br> 在 GitHub 上檢視\n",
                "    </a>\n",
                "  </td>\n",
                "</table>\n",
                "\n",
                "<div style=\"clear: both;\"></div>\n",
                "\n",
                "<b>分享到：</b>\n",
                "\n",
                "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/multimodal-live-api/intro_multimodal_live_api_genai_sdk.ipynb\" target=\"_blank\">\n",
                "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
                "</a>\n",
                "\n",
                "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/multimodal-live-api/intro_multimodal_live_api_genai_sdk.ipynb\" target=\"_blank\">\n",
                "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
                "</a>\n",
                "\n",
                "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/multimodal-live-api/intro_multimodal_live_api_genai_sdk.ipynb\" target=\"_blank\">\n",
                "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/53/X_logo_2023_original.svg\" alt=\"X logo\">\n",
                "</a>\n",
                "\n",
                "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/multimodal-live-api/intro_multimodal_live_api_genai_sdk.ipynb\" target=\"_blank\">\n",
                "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
                "</a>\n",
                "\n",
                "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/multimodal-live-api/intro_multimodal_live_api_genai_sdk.ipynb\" target=\"_blank\">\n",
                "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
                "</a>\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "84f0f73a0f76"
            },
            "source": [
                "| | |\n",
                "|-|-|\n",
                "| 作者 |  [Eric Dong](https://github.com/gericdong), [Holt Skinner](https://github.com/holtskinner) |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "tvgnzT1CKxrO"
            },
            "source": [
                "## 概述\n",
                "\n",
                "Multimodal Live API 支援與 Gemini 進行低延遲的雙向語音和視訊互動。該 API 可以處理文字、音訊和視訊輸入，並提供文字和音訊輸出。本教學示範了以下簡單範例，以協助您開始使用 Vertex AI 中的 Google Gen AI SDK 來使用 Multimodal Live API。\n",
                "\n",
                "- 文字生成文字\n",
                "- 文字生成音訊\n",
                "- 文字生成音訊對話\n",
                "- 函式呼叫\n",
                "- 程式碼執行\n",
                "- Google 搜尋\n",
                "\n",
                "如需更多詳細資訊，請參閱 [Multimodal Live API](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/multimodal-live) 頁面。"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "gPiTOAHURvTM"
            },
            "source": [
                "## 快速入門"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "CHRZUpfWSEpp"
            },
            "source": [
                "### Python 指令安裝 Google Gen AI SDK\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {
                "id": "sG3_LKsWSD3A"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Note: you may need to restart the kernel to use updated packages.\n"
                    ]
                }
            ],
            "source": [
                "%pip install --upgrade --quiet google-genai"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "HlMVjiAWSMNX"
            },
            "source": [
                "### 驗證你的 notebook 環境（Colab only）\n",
                "\n",
                "如果您在 Google Colab 上執行此 notebook，請執行下方的程式碼來看當前的環境驗證狀態。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {
                "id": "12fnq4V0SNV3"
            },
            "outputs": [],
            "source": [
                "import sys\n",
                "\n",
                "if \"google.colab\" in sys.modules:\n",
                "    from google.colab import auth\n",
                "\n",
                "    auth.authenticate_user()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "0Ef0zVX-X9Bg"
            },
            "source": [
                "### 匯入函式庫\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "xBCH3hnAX9Bh"
            },
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "from IPython.display import Audio, Markdown, display\n",
                "from google import genai\n",
                "from google.genai.types import (\n",
                "    FunctionDeclaration,\n",
                "    GoogleSearch,\n",
                "    LiveConnectConfig,\n",
                "    PrebuiltVoiceConfig,\n",
                "    SpeechConfig,\n",
                "    Tool,\n",
                "    ToolCodeExecution,\n",
                "    VoiceConfig,\n",
                ")\n",
                "import numpy as np "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "LymmEN6GSTn-"
            },
            "source": [
                "### 設定 Google Cloud 專案資訊並建立用戶端\n",
                "\n",
                "若要開始使用 Vertex AI，你必須有一個已[啟用 Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)服務的 Google Cloud 專案。\n",
                "\n",
                "更多關於請見[設定專案和開發環境](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "Nqwi-5ufWp_B"
            },
            "outputs": [],
            "source": [
                "PROJECT_ID = \"[your-project-id]\"  # @param {type: \"string\"}\n",
                "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
                "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
                "\n",
                "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {
                "id": "T-tiytzQE0uM"
            },
            "outputs": [],
            "source": [
                "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "5M7EKckIYVFy"
            },
            "source": [
                "### 使用 Gemini 2.0 Flash 模型\n",
                "\n",
                "Multimodal Live API 是一項透過引入 [Gemini 2.0 Flash 模型](https://cloud.google.com/vertex-ai/generative-ai/docs/gemini-v2) 來使用的新功能。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {
                "id": "-coEslfWPrxo"
            },
            "outputs": [],
            "source": [
                "MODEL_ID = \"gemini-2.0-flash-exp\"  # @param {type: \"string\"}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "b51c5ced31f7"
            },
            "source": [
                "## 使用 Multimodal Live API\n",
                "\n",
                "Multimodal Live API 是一個使用 [WebSockets](https://en.wikipedia.org/wiki/WebSocket) 的有狀態 API。本節展示了如何使用 Multimodal Live API 進行文字生成文字，和文字生成音訊的基本範例。"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Q1DE3s_LIUuE"
            },
            "source": [
                "### **範例 1**：文字生成文字\n",
                "\n",
                "傳送文字 prompt 會收到模型生成的文字訊息。\n",
                "\n",
                "**注意事項**\n",
                "- 會話（以下統一用原文`session`） 代表用戶端與伺服器之間的單一 WebSocket 連線。\n",
                "- 一個 seesion 的設定中，包含了模型、生成參數、系統指示和工具。\n",
                "  - `response_modalities` 接受 `TEXT` 或 `AUDIO`。\n",
                "- 在 session 初始化後，這個 session 可以傳遞訊息和伺服器互動\n",
                "  - 將文字、音訊或影片傳送到伺服器。\n",
                "  - 伺服器會在接收到音訊、文字或函式呼叫後回應。\n",
                "- 傳送訊息給伺服器時，將參數 `end_of_turn` 設定為 `True`，表示伺服器會以當前所累積 prompt 來生成內容。否則，伺服器會在開始生成前一直等待追加的訊息。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "NbJZzc7CIha5"
            },
            "outputs": [],
            "source": [
                "config = LiveConnectConfig(response_modalities=[\"TEXT\"])\n",
                "\n",
                "async with client.aio.live.connect(\n",
                "    model=MODEL_ID,\n",
                "    config=config,\n",
                ") as session:\n",
                "    text_input = \"Hello? Gemini are you there?\"\n",
                "    display(Markdown(f\"**Input:** {text_input}\"))\n",
                "\n",
                "    await session.send(input=text_input, end_of_turn=True)\n",
                "\n",
                "    response = []\n",
                "\n",
                "    async for message in session.receive():\n",
                "        if message.text:\n",
                "            response.append(message.text)\n",
                "\n",
                "    display(Markdown(f\"**Response >** {''.join(response)}\"))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "cG3346aA9sRR"
            },
            "source": [
                "### **範例 2**：文字生成音訊\n",
                "\n",
                "傳送文字 prompt 會收到模型生產的音訊。\n",
                "\n",
                "**注意事項**\n",
                "- Multimodal Live API 支援以下聲音種類：\n",
                "  - Puck\n",
                "  - Charon\n",
                "  - Kore\n",
                "  - Fenrir\n",
                "  - Aoede\n",
                "- 若要指定聲音，請在 `speech_config` 物件中設定 `voice_name`，作為 session 組態的一部分。\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "Iz3OkQ-a51QM"
            },
            "outputs": [],
            "source": [
                "config = LiveConnectConfig(\n",
                "    response_modalities=[\"AUDIO\"],\n",
                "    speech_config=SpeechConfig(\n",
                "        voice_config=VoiceConfig(\n",
                "            prebuilt_voice_config=PrebuiltVoiceConfig(\n",
                "                voice_name=\"Aoede\",\n",
                "            )\n",
                "        )\n",
                "    ),\n",
                ")\n",
                "\n",
                "async with client.aio.live.connect(\n",
                "    model=MODEL_ID,\n",
                "    config=config,\n",
                ") as session:\n",
                "    text_input = \"Hello? Gemini are you there?\"\n",
                "    display(Markdown(f\"**Input:** {text_input}\"))\n",
                "\n",
                "    await session.send(input=text_input, end_of_turn=True)\n",
                "\n",
                "    audio_data = []\n",
                "    async for message in session.receive():\n",
                "        if message.server_content.model_turn:\n",
                "            parts = message.server_content.model_turn.parts\n",
                "            if parts is not None: \n",
                "                for part in message.server_content.model_turn.parts:\n",
                "                    if part.inline_data:\n",
                "                        audio_data.append(\n",
                "                            np.frombuffer(part.inline_data.data, dtype=np.int16)\n",
                "                        )\n",
                "\n",
                "    if audio_data:\n",
                "        display(Audio(np.concatenate(audio_data), rate=24000, autoplay=True))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "JOBlWf566HOx"
            },
            "source": [
                "### **範例 3**：文字轉音訊對話\n",
                "\n",
                "**步驟 1**：您使用 API 建立交談時，你可以使用傳送文字 prompts 並接收音訊回應。\n",
                "\n",
                "**注意事項**\n",
                "\n",
                "- 雖然模型會追會話中（in-session）的互動，但目前無法透過 API 存取明確的 session 記錄。當 session 終止時，當前的的內容會被刪除。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "bhY0P0qpRP5y"
            },
            "outputs": [],
            "source": [
                "config = LiveConnectConfig(response_modalities=[\"AUDIO\"])\n",
                "\n",
                "\n",
                "async def main() -> None:\n",
                "    async with client.aio.live.connect(model=MODEL_ID, config=config) as session:\n",
                "\n",
                "        async def send() -> bool:\n",
                "            text_input = input(\"輸入 > \")\n",
                "            if text_input.lower() in (\"q\", \"quit\", \"exit\"):\n",
                "                return False\n",
                "            await session.send(input=text_input, end_of_turn=True)\n",
                "            return True\n",
                "\n",
                "        async def receive() -> None:\n",
                "\n",
                "            audio_data = []\n",
                "\n",
                "            async for message in session.receive():\n",
                "                parts = message.server_content.model_turn.parts\n",
                "                if parts is not None: \n",
                "                    if message.server_content.model_turn:\n",
                "                        for part in message.server_content.model_turn.parts:\n",
                "                            if part.inline_data:\n",
                "                                audio_data.append(\n",
                "                                    np.frombuffer(part.inline_data.data, dtype=np.int16)\n",
                "                                )\n",
                "\n",
                "                if message.server_content.turn_complete:\n",
                "                    display(Markdown(\"**回應 >**\"))\n",
                "                    display(\n",
                "                        Audio(np.concatenate(audio_data), rate=24000, autoplay=True)\n",
                "                    )\n",
                "                    break\n",
                "\n",
                "            return\n",
                "\n",
                "        while True:\n",
                "            if not await send():\n",
                "                break\n",
                "            await receive()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "94IeUUb3e90M"
            },
            "source": [
                "**步驟 2** 執行聊天，輸入您的提示，或輸入 `q`、`quit` 或 `exit` 退出。\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "2UvgUDIYJqfw"
            },
            "outputs": [],
            "source": [
                "await main()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "907da7836dcf"
            },
            "source": [
                "### **範例 4**：函式呼叫\n",
                "\n",
                "您可以建立的描述後，使用函式呼叫（Function calling）將該描述傳送給模型。模型則會根據這個函式裡的名稱以及參數來使用它。\n",
                "\n",
                "**注意事項**：\n",
                "\n",
                "- 所有函式的宣告，必須在執行 session 將 tool 傳送前 定義。\n",
                "- 目前 API 只支援一個 tool。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "0f4657af21e3"
            },
            "outputs": [],
            "source": [
                "get_current_weather = FunctionDeclaration(\n",
                "    name=\"get_current_weather\",\n",
                "    description=\"Get current weather in the given location\",\n",
                "    parameters={\n",
                "        \"type\": \"OBJECT\",\n",
                "        \"properties\": {\n",
                "            \"location\": {\n",
                "                \"type\": \"STRING\",\n",
                "            },\n",
                "        },\n",
                "    },\n",
                ")\n",
                "\n",
                "config = LiveConnectConfig(\n",
                "    response_modalities=[\"TEXT\"],\n",
                "    tools=[Tool(function_declarations=[get_current_weather])],\n",
                ")\n",
                "\n",
                "async with client.aio.live.connect(\n",
                "    model=MODEL_ID,\n",
                "    config=config,\n",
                ") as session:\n",
                "    text_input = \"Get the current weather in Santa Clara, San Jose and Mountain View\"\n",
                "    display(Markdown(f\"**Input:** {text_input}\"))\n",
                "\n",
                "    await session.send(input=text_input, end_of_turn=True)\n",
                "\n",
                "    async for message in session.receive():\n",
                "        if message.tool_call:\n",
                "            for function_call in message.tool_call.function_calls:\n",
                "                display(Markdown(f\"**FunctionCall >** {str(function_call)}\"))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "23cb7ab89311"
            },
            "source": [
                "### **範例 5**：程式碼執行\n",
                "\n",
                " 你可以直接使用 API 中的程式碼執行（Code Execution）功能，直接生成 Python 程式碼並執行它。\n",
                "\n",
                " 在此範例中，你在初始化程式碼執行工具時，是在 `Tool` 中使用 `code_execution`。並且將它和模型設定一起註冊給 session。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "631b0c59c516"
            },
            "outputs": [],
            "source": [
                "config = LiveConnectConfig(\n",
                "    response_modalities=[\"TEXT\"],\n",
                "    tools=[Tool(code_execution=ToolCodeExecution())],\n",
                ")\n",
                "\n",
                "async with client.aio.live.connect(\n",
                "    model=MODEL_ID,\n",
                "    config=config,\n",
                ") as session:\n",
                "    text_input = \"Write code to calculate the 15th fibonacci number then find the nearest palindrome to it\"\n",
                "    display(Markdown(f\"**Input:** {text_input}\"))\n",
                "\n",
                "    await session.send(input=text_input, end_of_turn=True)\n",
                "\n",
                "    response = []\n",
                "\n",
                "    async for message in session.receive():\n",
                "        if message.text:\n",
                "            response.append(message.text)\n",
                "        if message.server_content.model_turn.parts:\n",
                "            for part in message.server_content.model_turn.parts:\n",
                "                if part.executable_code:\n",
                "                    display(\n",
                "                        Markdown(\n",
                "                            f\"\"\"\n",
                "**Executable code:**\n",
                "```py\n",
                "{part.executable_code.code}\n",
                "```\n",
                "\"\"\"\n",
                "                        )\n",
                "                    )\n",
                "\n",
                "    display(Markdown(f\"**Response >** {''.join(response)}\"))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "73660342318d"
            },
            "source": [
                "### **範例 6**：Google 搜尋\n",
                "\n",
                "`google_search` 工具可讓模型進行 Google 搜尋。例如，可以嘗試詢問它，近期還沒有被納入訓練資料中的事件。\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "e64fc3a94d49"
            },
            "outputs": [],
            "source": [
                "config = LiveConnectConfig(\n",
                "    response_modalities=[\"TEXT\"],\n",
                "    tools=[Tool(google_search=GoogleSearch())],\n",
                ")\n",
                "\n",
                "async with client.aio.live.connect(\n",
                "    model=MODEL_ID,\n",
                "    config=config,\n",
                ") as session:\n",
                "    text_input = (\n",
                "        \"Tell me about the largest earthquake in California the week of Dec 5 2024?\"\n",
                "    )\n",
                "    display(Markdown(f\"**Input:** {text_input}\"))\n",
                "\n",
                "    await session.send(input=text_input, end_of_turn=True)\n",
                "\n",
                "    response = []\n",
                "\n",
                "    async for message in session.receive():\n",
                "        if message.text:\n",
                "            response.append(message.text)\n",
                "\n",
                "    display(Markdown(f\"**Response >** {''.join(response)}\"))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "usjiqTDXfk_6"
            },
            "source": [
                "## 接下來\n",
                "\n",
                "- 學習如何[建立 Web 應用程式，讓你可以使用聲音和相機，透過 Multimodal Live API 與 Gemini 2.0 對話。](https://github.com/GoogleCloudPlatform/generative-ai/tree/main/gemini/multimodal-live-api/websocket-demo-app)\n",
                "- 參閱 [Multimodal Live API 參考文件](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/multimodal-live)。\n",
                "- 參閱 [Google Gen AI SDK 參考文件](https://googleapis.github.io/python-genai/)。\n",
                "- 探索 [Google Cloud 產生式 AI GitHub 儲存庫](https://github.com/GoogleCloudPlatform/generative-ai) 中的其他 notebook。"
            ]
        }
    ],
    "metadata": {
        "colab": {
            "name": "intro_multimodal_live_api_genai_sdk.ipynb",
            "toc_visible": true
        },
        "kernelspec": {
            "display_name": "3.10.14",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
